{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f224dfef",
   "metadata": {
    "id": "f224dfef"
   },
   "source": [
    "# Creating Synthetic Experts with Generative AI\n",
    "> ## Label Text with ChatGPT4\n",
    "\n",
    "Version 1.1   \n",
    "Date: September 2, 2023    \n",
    "Author: Daniel M. Ringel   \n",
    "Contact: dmr@unc.edu   \n",
    "\n",
    "*Daniel M. Ringel, Creating Synthetic Experts with Generative Artificial Intelligence (July 15, 2023).  \n",
    "Available at SSRN: https://papers.ssrn.com/abstract_id=4542949*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0807e5",
   "metadata": {},
   "source": [
    "#### This notebook uses the OpenAI API to communicate with GPT4.\n",
    "- You need an account with OpenAI for API access\n",
    "- Visit https://platform.openai.com/signup?launch to sign-up\n",
    "- Beware that using the API comes at a cost: https://openai.com/pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761c930",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b2f6e4",
   "metadata": {
    "executionInfo": {
     "elapsed": 2506,
     "status": "ok",
     "timestamp": 1683807405135,
     "user": {
      "displayName": "Doctor D at UNC",
      "userId": "03996805680806580115"
     },
     "user_tz": 240
    },
    "id": "b3b2f6e4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import re, os, signal, datetime, warnings\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "pd.set_option('display.max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c4e44",
   "metadata": {
    "id": "b30c4e44"
   },
   "source": [
    "# 2. Configure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0472962",
   "metadata": {},
   "source": [
    "##### By using this notebook, you agree that the author is not liable for any cost or damages that you incur.\n",
    "\n",
    "> I ***strongly recommend*** that you set a ***soft limit*** and a ***hard limit*** on your ***OpenAI account*** before running this notebook to prevent excessive cost due to glitches in the API interaction (e.g., unexpected answers from the API lead to ongoing queries that incur cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "790ec965",
   "metadata": {
    "id": "790ec965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! Your API Key is included in this notebook !!!\n",
      "\n",
      " >>> Do not forget to delete it before you share the notebook <<<\n"
     ]
    }
   ],
   "source": [
    "# Put your OpenAI API Key here. DO NOT SHARE YOUR KEY! \n",
    "# ----> Always delete your OpenAi API key before sharing the notebook! <-------\n",
    "\n",
    "api_key = \"DEMO-DGDH4Rd4gfsdhRRFgdsgh23rEdsGg3hyEAAFG12SFysd\"\n",
    "\n",
    "if not api_key == None:\n",
    "    print(\"!!! Your API Key may be included in this notebook !!!\\n\\n >>> Do not forget to delete it before you share the notebook <<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433acb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Paths\n",
    "IN_Path = \"Data\"\n",
    "IN_File = \"Demo_250_SyntheticTwins\"\n",
    "TEMP_Path = \"tmp\"\n",
    "TEMP_File = \"SyntheticExperts_tmp\"\n",
    "OUT_Path = \"Data/out\"\n",
    "OUT_File = \"Labeled-Texts\"\n",
    "\n",
    "if not os.path.exists(TEMP_Path): os.makedirs(TEMP_Path)\n",
    "if not os.path.exists(OUT_Path): os.makedirs(OUT_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb97cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formulate AI Prompt in RTF (Role, Task, Format) convention\n",
    "AI_Prompt = \"You are a renowned marketing scholar and an expert on the 4 Ps of Marketing: Product, Place, Price, and Promotion. When given a numbered list of Tweets, you examine each Tweet individually. For each Tweet, determine which of the 4 Ps it is about, if any. Output all relevant Ps for each tweet. Use only the terms Product, Place, Price, and Promotion. Do not provide notes or an explanation.\"\n",
    "\n",
    "# AI Controls\n",
    "tokens = int(2000)\n",
    "temp = 0 # According to OpenAI, as this value approaches 0, the GPT4 model becomes deterministic in its responses\n",
    "model = \"gpt-4\"\n",
    "\n",
    "# Batch Controls (number of texts per query - need consider token limits, cost of retries, and size of failed batches)\n",
    "batch_size = 25\n",
    "\n",
    "# Set random state (change this for each run when you take majority labels across multiple runs; see Ringel (2023))\n",
    "seed = 76"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139bba37",
   "metadata": {},
   "source": [
    "**Notes on batch size:** \n",
    "\n",
    "- At the time of developing this notebook, the performance of OpenAI's API varied dramatically by \n",
    "> *weekday* **x** *time of day* **x** *internet connection* **x** *model used* **x** *number of tokens* \n",
    "- In general, I found:\n",
    "    - smaller batches were less prone to API communication errors than larger batches.\n",
    "    - longer texts work better in smaller batches\n",
    "    - runtime dramatically increases during business hours\n",
    "    - format of AI response deviates more during business hours and early evening, which can lead to errors in response processing\n",
    "    \n",
    "***My take-away:*** Create Synthetic Experts overnight on weekends and keep batch size at moderate level, especially for longer texts (i.e., more tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b401b2",
   "metadata": {},
   "source": [
    "# 3. Helper Functions\n",
    "\n",
    "**Note from author:** These functions are coded for functionality, not for speed, elegance, or best readability at (i.e., not fully pythonic). Refactor them as needed.\n",
    "\n",
    "The code in the function *twins_from_ai* is rather extensive to catch errors, retry queries, and collect failed batches. While shorter solutions are possible, I found that the current state of OpenAI's API and models calls for extensive error catching and handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa526a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query(dataframe, start=0, end=0):\n",
    "    \"\"\"Function that builds the AI_query\"\"\"\n",
    "    AI_Query = \"\".join([f\"{i}: {dataframe.iloc[i]['Text']}\\n\" for i in range(start, end+1)])\n",
    "    return AI_Query\n",
    "\n",
    "def handle_interrupt(signal, frame):\n",
    "    \"\"\"Function to handle interrupts\"\"\"\n",
    "    print(\"Interrupt signal received. Exiting...\")\n",
    "    exit(0)\n",
    "\n",
    "def ask_gpt(AI_Prompt, AI_Query, tokens=2000, temp=1, model=\"gpt-4\"):\n",
    "    \"\"\"Function that Queries OpenAI API\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": AI_Prompt},\n",
    "            {\"role\": \"user\", \"content\": AI_Query}],\n",
    "        max_tokens=tokens,\n",
    "        temperature=temp,)\n",
    "    return response\n",
    "\n",
    "def process_response(answer, start, end, retry_count):\n",
    "    \"\"\"Function that Processes AI Response. Note: For the original project about marketing mix variables, 'FourP' corresponds to the returned labels. \"\"\"\n",
    "    # Extract content from answer\n",
    "    if 'message' in answer.choices[0]:\n",
    "        answer_content = answer.choices[0].message.content\n",
    "    elif 'text' in answer.choices[0]:\n",
    "        answer_content = answer.choices[0].text\n",
    "    else: \n",
    "        raise ValueError(\"Processing Error: Cannot find model response text\")\n",
    "    # Get the token usage\n",
    "    used_tokens = answer['usage']['total_tokens']\n",
    "    # Pre-process message content\n",
    "    # answer_content = answer_content.replace(\"###\", \"\") #optional if you are passing additional information behind a separator (e.g., ###)\n",
    "    lines = [line.strip() for line in answer_content.split('\\n') if line.strip()]\n",
    "    results = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            index = int(re.findall(r'^(\\d+)', line.strip())[0])\n",
    "            text = re.findall(r'^\\d+[:.\\s](.*)$', line.strip())[0].strip()\n",
    "        except IndexError:\n",
    "            continue\n",
    "        if retry_count < 3:\n",
    "            if index is None:\n",
    "                raise ValueError(\"Response missing [index]\")\n",
    "            if text is None or len(text) == 0:\n",
    "                raise ValueError(\"Response missing [text]\")\n",
    "        results.append((index, text))  \n",
    "    if len(results) == 0:\n",
    "        raise ValueError(\"No index returned with content\")\n",
    "    # Create DataFrame\n",
    "    FourP = pd.DataFrame(results, columns=[\"Index\", \"Content\"]).set_index('Index', drop=True).rename_axis(None)\n",
    "    FourP = FourP[~FourP.index.duplicated(keep='first')]\n",
    "    # Check if the indices are within the range [start, end]\n",
    "    indices = FourP.index.tolist()\n",
    "    if not all(start <= index <= end for index in indices):\n",
    "        raise ValueError(\"Returned indices do not correspond to input indices\")    \n",
    "    return FourP, used_tokens\n",
    "\n",
    "def classify_by_ai(AI_Prompt, batch_size, model, tokens, temp, data, interims_file):\n",
    "    \"\"\"Function that gets synthetic twins of text from AI\"\"\"\n",
    "    counter, sum_tokens, consecutive_fails = 1, 0, 0\n",
    "    data_len = len(data)\n",
    "    num_full_batches, remainder = divmod(data_len, batch_size)\n",
    "    failed_batches = pd.DataFrame(columns=['start', 'end'])  # DataFrame to store the failed batches\n",
    "    signal.signal(signal.SIGINT, handle_interrupt)    \n",
    "    def process_batch(start, end, max_tries=5):\n",
    "        nonlocal consecutive_fails, sum_tokens, counter\n",
    "        print(f\"\\nstart: {start}, end: {end}\")\n",
    "        AI_Query = build_query(data, start, end)\n",
    "        #signal.signal(signal.SIGINT, handle_interrupt) #optional for local handling (set to global 5 lines earlier)\n",
    "        tries_query = 0\n",
    "        while tries_query < max_tries:\n",
    "            try:\n",
    "                print(datetime.datetime.now(), f\"Querying OpenAI: Try {tries_query+1}\")\n",
    "                if \"gpt\" in model:\n",
    "                    response = ask_gpt(AI_Prompt, AI_Query, tokens, temp, model)\n",
    "                else:\n",
    "                    print(\"Unknown Model Specification\")\n",
    "                try:\n",
    "                    FourP, used_tokens = process_response(response, start, end, tries_query)\n",
    "                    if 'Content' not in FourP.columns:\n",
    "                        raise ValueError(\"Expected 'Content' column in FourP DataFrame\")                    \n",
    "                    sum_tokens += used_tokens\n",
    "                    data.loc[FourP.index, '4P'] = FourP['Content'].values\n",
    "                    consecutive_fails = 0\n",
    "                    return True\n",
    "                except ValueError as ve:\n",
    "                    print(f\"Unexpected AI response. Try {tries_query+1}, Error: {ve}\")\n",
    "                    tries_query += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                tries_query += 1\n",
    "        print(f\"Failed querying OpenAI {max_tries} times at batch {counter}.\")\n",
    "        consecutive_fails += 1\n",
    "        new_row = {'start': start, 'end': end}\n",
    "        failed_batches.loc[counter] = new_row\n",
    "        return False\n",
    "    for batch_num in range(num_full_batches):\n",
    "        if consecutive_fails == 5:\n",
    "            print(\"5 consecutive fails encountered. Stopping the process.\")\n",
    "            return data, sum_tokens, failed_batches\n",
    "        start, end = batch_num * batch_size, (batch_num + 1) * batch_size - 1\n",
    "        process_batch(start, end)\n",
    "        if counter % 10 == 0:\n",
    "            data.to_pickle(f\"{interims_file}.pkl\")\n",
    "            failed_batches.to_pickle(f\"{interims_file}_Failed_batches.pkl\")\n",
    "            print(f\"Interim Results Saved: Batch {counter}\")\n",
    "        counter += 1\n",
    "        print(f\"Total Tokens used so far: {sum_tokens}\")\n",
    "    if remainder >= 2:\n",
    "        start, end = num_full_batches * batch_size, num_full_batches * batch_size + remainder - 1\n",
    "        process_batch(start, end, max_tries=3)\n",
    "        if counter % 10 == 0:\n",
    "            data.to_pickle(f\"{interims_file}.pkl\")\n",
    "            failed_batches.to_pickle(f\"{interims_file}_Failed_batches.pkl\")\n",
    "            print(f\"Interim Results Saved: Batch {counter}\")        \n",
    "        print(f\"Total Tokens used so far: {sum_tokens}\")\n",
    "    return data, sum_tokens, failed_batches\n",
    "\n",
    "def clean_and_parse_text(text):\n",
    "    \"\"\"Function that cleans-up texts by (1) parsing HTML, (2) removing URLS (replace with URL), (3) removing line breaks and leading periods and colons, and (4) removing leading, trailing, and duplicate spaces.\"\"\"\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" URL \", text)\n",
    "    parsed = BeautifulSoup(text, \"html.parser\").get_text() if \"filename\" not in str(BeautifulSoup(text, \"html.parser\")) else None\n",
    "    return re.sub(r\" +\", \" \", re.sub(r'^[.:]+', '', re.sub(r\"\\\\n+|\\n+\", \" \", parsed or text)).strip()) if parsed else None\n",
    "\n",
    "def boolean_ps(frame):\n",
    "    \"\"\"Function that checks which Ps the AI identified and creates a Boolean column for each P\"\"\"\n",
    "    for p in ['Product', 'Place', 'Price', 'Promotion']:\n",
    "        frame[p] = frame['4P'].apply(\n",
    "            lambda x: any([\n",
    "                p.lower() in item.lower() \n",
    "                for item in (x.split(\", \") if isinstance(x, str) else x)]))\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade34cc3",
   "metadata": {},
   "source": [
    "# 2. Load, Parse, and Clean Demo Text\n",
    "These 250 demo texts are ***Synthetic Twins*** of real Tweets. I do not publish real (i.e., original) Tweets with this notebook.  \n",
    "> ***Synthetic Twins*** correspond semantically in idea and meaning to original texts. However, wording, people, places, firms, brands, and products were changed by an AI. As such, ***Synthetic Twins*** mitigate, to some extent, possible privacy, and copyright concerns. If you'd like to learn more about ***Synthetic Twins***, another generative AI project by Daniel Ringel, then please get in touch! dmr@unc.edu  \n",
    "\n",
    "\n",
    "You can ***create your own Synthetic Twins of texts*** with this Python notebook:   `SyntheticExperts_Create_Synthetic_Twins_of_Texts.ipynb`,   \n",
    "available as BETA version (still being tested) on the **Synthetic Experts [GitHub](https://github.com/dringel/Synthetic-Experts)** respository.<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9cb9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wd/5r_mh08d50nbkshrgqnvc_qw0000gq/T/ipykernel_79871/142412283.py:124: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  parsed = BeautifulSoup(text, \"html.parser\").get_text() if \"filename\" not in str(BeautifulSoup(text, \"html.parser\")) else None\n"
     ]
    }
   ],
   "source": [
    "# Load Texts\n",
    "df = pd.read_pickle(f\"{IN_Path}/{IN_File}.pkl\")\n",
    "df['Text'] = df['Text'].apply(clean_and_parse_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af8be4",
   "metadata": {},
   "source": [
    "You may see a warning from Beautiful Soup when it finds a pattern in text that is similar to a filename. This warning is not a problem for this notebook and for what we are doing here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2b4b4",
   "metadata": {},
   "source": [
    "# 3. Label Text with OpenAI's GPT4\n",
    "\n",
    "> From my experience, the speed at which the AI labels texts, and the occurrence of possible errors in communicating with the API is related to the day and time of day you query the API. Workday afternoons and evenings tend to see more traffic (i.e., queries) to GPT4, which can slow down its responses, lead to time-outs, and create various other errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c5ed3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up OpenAI API Key\n",
    "openai.api_key = None\n",
    "openai.api_key = api_key\n",
    "\n",
    "# Shuffle order while preserving Index\n",
    "df[\"original_Index\"] = df.index\n",
    "df = df.sample(frac=1, random_state=seed)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba190bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start: 0, end: 24\n",
      "2023-08-25 10:32:22.412614 Querying OpenAI: Try 1\n",
      "Total Tokens used so far: 1501\n",
      "\n",
      "start: 25, end: 49\n",
      "2023-08-25 10:32:40.588612 Querying OpenAI: Try 1\n",
      "Total Tokens used so far: 3028\n",
      "\n",
      "start: 50, end: 74\n",
      "2023-08-25 10:32:54.851895 Querying OpenAI: Try 1\n",
      "Total Tokens used so far: 4593\n",
      "\n",
      "start: 75, end: 99\n",
      "2023-08-25 10:33:10.620880 Querying OpenAI: Try 1\n",
      "Total Tokens used so far: 6125\n",
      "\n",
      "start: 100, end: 124\n",
      "2023-08-25 10:33:26.595477 Querying OpenAI: Try 1\n",
      "Total Tokens used so far: 7765\n",
      "\n",
      "start: 125, end: 149\n",
      "2023-08-25 10:33:42.059876 Querying OpenAI: Try 1\n",
      "Total Tokens used so far: 9394\n",
      "\n",
      "start: 150, end: 174\n",
      "2023-08-25 10:33:56.395934 Querying OpenAI: Try 1\n",
      "Total Tokens used so far: 10963\n",
      "\n",
      "start: 175, end: 199\n",
      "2023-08-25 10:34:12.059762 Querying OpenAI: Try 1\n",
      "Total Tokens used so far: 12640\n",
      "\n",
      "start: 200, end: 224\n",
      "2023-08-25 10:34:31.620215 Querying OpenAI: Try 1\n",
      "Total Tokens used so far: 14301\n",
      "\n",
      "start: 225, end: 249\n",
      "2023-08-25 10:34:46.568102 Querying OpenAI: Try 1\n",
      "Interim Results Saved: Batch 10\n",
      "Total Tokens used so far: 15970\n",
      "\n",
      "Complete. Total tokens used: 15970\n"
     ]
    }
   ],
   "source": [
    "# Label with GPT-4\n",
    "interims_file = f\"{TEMP_Path}/{TEMP_File}_seed{seed}\"\n",
    "out, total_tokens, failed_batches = classify_by_ai(AI_Prompt, batch_size, model, tokens, temp, df, interims_file)\n",
    "print(f\"\\nComplete. Total tokens used: {total_tokens}\")\n",
    "if not failed_batches.empty:\n",
    "    print(f\"WARNING: AI failed to label {len(failed_batches)} rows (texts).\\nConsider querying the AI again for just these rows (texts).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47960981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>original_Index</th>\n",
       "      <th>4P</th>\n",
       "      <th>Product</th>\n",
       "      <th>Place</th>\n",
       "      <th>Price</th>\n",
       "      <th>Promotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today, I somehow ended up eating a full box of @Triscuit and according to my mom, I'm going to be \"Triscuit Tubby\" ü§£ü§¶‚Äç‚ôÄÔ∏è</td>\n",
       "      <td>91</td>\n",
       "      <td>Product</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Congratulations to the well-deserving duo who received an award for their outstanding support of @StJude. @BeyonceOfficial #GivingBack</td>\n",
       "      <td>235</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We may have another month of a highly positive jobs report, over a million perhaps, @POTUS better hope numbers don't normalize before next month @BBCNews @rtenews @SkyNews @CNNEE @CBSNews @nbc</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Craving the perfect pizza? Look no further! Experience pizza perfection with our mouthwatering creations. Leave behind the mainstream choices and let us treat you to a pizza like no other. Call us today and enjoy a truly satisfying pizza experience. üçïüòã URL #PizzaPerfection #IndulgeInDeliciousness</td>\n",
       "      <td>7</td>\n",
       "      <td>Product, Place</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If @Nokia brought back the classic Symbian phone, I would be the first one in the queue to purchase it. Regardless of the price, I would definitely get it. Shut up and take my money.</td>\n",
       "      <td>113</td>\n",
       "      <td>Product, Price</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                        Text   \n",
       "0                                                                                                                                                                                   Today, I somehow ended up eating a full box of @Triscuit and according to my mom, I'm going to be \"Triscuit Tubby\" ü§£ü§¶‚Äç‚ôÄÔ∏è  \\\n",
       "1                                                                                                                                                                     Congratulations to the well-deserving duo who received an award for their outstanding support of @StJude. @BeyonceOfficial #GivingBack   \n",
       "2                                                                                                           We may have another month of a highly positive jobs report, over a million perhaps, @POTUS better hope numbers don't normalize before next month @BBCNews @rtenews @SkyNews @CNNEE @CBSNews @nbc   \n",
       "3  Craving the perfect pizza? Look no further! Experience pizza perfection with our mouthwatering creations. Leave behind the mainstream choices and let us treat you to a pizza like no other. Call us today and enjoy a truly satisfying pizza experience. üçïüòã URL #PizzaPerfection #IndulgeInDeliciousness   \n",
       "4                                                                                                                     If @Nokia brought back the classic Symbian phone, I would be the first one in the queue to purchase it. Regardless of the price, I would definitely get it. Shut up and take my money.   \n",
       "\n",
       "   original_Index              4P  Product  Place  Price  Promotion  \n",
       "0              91         Product     True  False  False      False  \n",
       "1             235            None    False  False  False      False  \n",
       "2              18            None    False  False  False      False  \n",
       "3               7  Product, Place     True   True  False      False  \n",
       "4             113  Product, Price     True  False   True      False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 4Ps to Boolean Columns\n",
    "out = boolean_ps(out)\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "417f7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct original index and recode:\n",
    "out = out.set_index('original_Index').sort_index(ascending=True).rename_axis(None)\n",
    "\n",
    "# Save\n",
    "out.to_pickle(f\"{OUT_Path}/{OUT_File}_seed{seed}_{model}_run1.pkl\")\n",
    "failed_batches.to_pickle(f\"{OUT_Path}/{OUT_File}_seed{seed}_{model}_failed_run1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1ebd36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use this notebook's code, please give credit to the author by citing the paper:\n",
      "\n",
      "Daniel M. Ringel, Creating Synthetic Experts with Generative Artificial Intelligence (July 15, 2023).Available at SSRN: https://papers.ssrn.com/abstract_id=4542949\n"
     ]
    }
   ],
   "source": [
    "print(\"If you use this notebook's code, please give credit to the author by citing the paper:\\n\\nDaniel M. Ringel, Creating Synthetic Experts with Generative Artificial Intelligence (July 15, 2023).\\nAvailable at SSRN: https://papers.ssrn.com/abstract_id=4542949\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
